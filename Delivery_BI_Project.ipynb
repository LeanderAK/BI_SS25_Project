{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "779cf0de",
   "metadata": {},
   "source": [
    "# BI Project - Breast Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9a4e95",
   "metadata": {},
   "source": [
    "## **Content of the Notebook**\n",
    "\n",
    "Procedure according to CRISP-DM Framework.\n",
    "\n",
    "The steps Data Understanding to Evaluation are illustrated in the notebook, all other contents are dealt with in the written elaboration.\n",
    "\n",
    "- Business Understanding\n",
    "- **Data Understanding**\n",
    "- **Data Preperation**\n",
    "- **Modeling**\n",
    "- **Evaluation**\n",
    "- Deployment Options & Future Outlook\n",
    "- Conclusion\n",
    "\n",
    "\n",
    "\n",
    "*The content and structure of the notebook, as well as definitions and explanations, are strongly based on the “Applied Analytics” course*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f70e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956d58b",
   "metadata": {},
   "source": [
    "## 1. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb4077",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "- Describe the dataset used\n",
    "- Explore and describe features\n",
    "- Discuss data quality (e.g., missing values)\n",
    "- Highlight patterns or trends\n",
    "- Include visualizations to support your data understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159035d8",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8191650f",
   "metadata": {},
   "source": [
    "Data Structure:\n",
    "\n",
    "- 1 Table\n",
    "- CSV-Format \n",
    "- Each feature has 2 linked/related features (except for the target variable and the ID) - e.g. the Radius variable has 3 columns: Mean, Standard Erros and Worst Radius\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a100702",
   "metadata": {},
   "source": [
    "Data Content:\n",
    "- Number of observations & variables\n",
    "- Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae34d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1567da5",
   "metadata": {},
   "source": [
    "Numerical/contiuos variables\n",
    "- Distribution\n",
    "- Min, Max, Mean, Median\n",
    "- Standard Deviation\n",
    "- Skewness\n",
    "- Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73b29c",
   "metadata": {},
   "source": [
    "Visualization\n",
    "- Plot data (Unidimensional (Histograms) and Multidemensional(Scatterplots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae080884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "# - Plot data (Unidimensional (Histograms) and Multidemensional(Scatterplots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91231058",
   "metadata": {},
   "source": [
    "EDA (Automated Exploratory Data Analysis)\n",
    "- sweetwiz\n",
    "- ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA \n",
    "# choose sweetviz or ydata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf3682",
   "metadata": {},
   "source": [
    "## 2. Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c4e6b",
   "metadata": {},
   "source": [
    "**Task**\n",
    "- Detail how you selected and filtered your data\n",
    "- Explain any transformations or feature engineering steps (including missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d28e1f",
   "metadata": {},
   "source": [
    "Variable Cleaning\n",
    "- Incorrect Values (True instead of 1, 1.000 instead of 1000 → else remove observations)\n",
    "- Consistency in Data (same format, check time and dates, ..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f694a2b",
   "metadata": {},
   "source": [
    "Outliers\n",
    "- Find 1. Point (univariate) outliers or/and 2. Contextual (multivariate) outliers\n",
    "- Methods to detect outliers: Tukey's Fence and Standardization / 3DS Method\n",
    "- Methods to detect multivariate outliers: Mahalanobis distance, PCA-based techniques, Robuts Covariance Estimation\n",
    "- Handle outliers: remove, transform, ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48e610",
   "metadata": {},
   "source": [
    "Missing Data\n",
    "- Missing completely at random (MCAR)\n",
    "- Missing at random (MAR)\n",
    "- Missing not at random (MNAR)\n",
    "\n",
    "How to deal with missing data:\n",
    "- Mean/Median Imputation\n",
    "- Hot-Deck / Distribution\n",
    "- Model based (eg. regression model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c37b7c",
   "metadata": {},
   "source": [
    "Data Wrangling\n",
    "- Data must be trimmed, reshaped, transformed, aggregated, merged, …\n",
    "- Data removal of irrelevant variables\n",
    "- Data transformations:\n",
    "    - Existing data to new variables (if-else rules, eg. if m → male, elif f → female, else diverse)\n",
    "    - Binning data: Continuous data to ordinal data (e.g. cluster heights)\n",
    "    - Distribution transformation: Skewed data can lead to bias → (log-) Transforming the variables\n",
    "- Reshaping data: more than one data point per unit of observations (wide to long or long to wide)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92695cac",
   "metadata": {},
   "source": [
    "Dimensionality Reduction\n",
    "- Goal: remove dimensions -> Curse of dimensionality\n",
    "- Remove redundancy and noise from the dataset\n",
    "- Techniques:\n",
    "    - Combining Features\n",
    "    - Principal component analyis (PCA)\n",
    "    - Factor analysis (FA)\n",
    "    - Singular value decomposition (SVD)\n",
    "    - Linear discriminant (LDA)\n",
    "\n",
    "PCA in detail:\n",
    "- Goal: Combine existing, correlated variables into less, independent variables\n",
    "- Process:\n",
    "    - Start with N dimensions in data set\n",
    "    - Normalize Data\n",
    "        - Min-Max (uniform distr.)\n",
    "        - Z-score scaling (gaussian distr.)\n",
    "        - Log transformation (skewness)\n",
    "    - Calculate N principal components (PCs) from data set\n",
    "        - Each PC explains a part of the variance ( → eigenvalue)\n",
    "        - Each PC receives a contribution to each other ( → eigenvector)\n",
    "    - Select the PCs with high eigenvalues; ignore the X remaining PCs\n",
    "- Analyze loadings / eigenvalues of the PCs (Feature Loadings on PCs Heat Map)\n",
    "- When to use PCAs:\n",
    "    - Too many variables → Curse of Dimensionality\n",
    "    - The features in the data are highly correlated → Multicollinearity\n",
    "    - Visualization of high-dimensional data → Visualize relevant PCs\n",
    "    - Noise reduction → Getting rid of low-variance components\n",
    "    - In combination with clustering algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f72a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction (1)\n",
    "# -- Combining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dfdd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction (2)\n",
    "# -- PCA\n",
    "# -- Evaluate PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ececf820",
   "metadata": {},
   "source": [
    "Clustering\n",
    "- K-Means\n",
    "    - Evaluation: \n",
    "        - Sum of squared Errors (SSE)\n",
    "        - Silhouette Score\n",
    "    - Optimal number of clusters:\n",
    "        - Elbow method\n",
    "\n",
    "- Optional: Kohonen Self Organizing Maps Algorithm (SOM - run a clustering algorithm (e.g. K-Means) on the weight vectors of SOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb3b2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd94841",
   "metadata": {},
   "source": [
    "Sampling\n",
    "- Partition data in training (70% - 80%) and testing data (20% - 30%)\n",
    "- Methods\n",
    "    - k-fold cross validation:\n",
    "        - partition data in k random samples\n",
    "        - train model on k-1 of the subsamples\n",
    "        - evaluate the k-1 models on the remaining testing subsample → error distribution indicates how stable the training is across data samples\n",
    "\n",
    "    - bootstrapping:\n",
    "        - From a dataset with N observations you sample N-times with replacement\n",
    "        - some of the original observations are multiple times in the sampled data - others not at all\n",
    "        - repeat a large number of times\n",
    "        - used to sample training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed99c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f8b44",
   "metadata": {},
   "source": [
    "## 3. Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193706cd",
   "metadata": {},
   "source": [
    "**Task**\n",
    "- Describe two different models selected for analysis\n",
    "- Describe how each model was implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5eeac8",
   "metadata": {},
   "source": [
    "- Taxanomic Overview\n",
    "- Predictive Modeling vs. Descriptive Modeling\n",
    "\n",
    "- **Predictive Modeling**\n",
    "  - Goal: forecast future outcomes\n",
    "  - Regression:\n",
    "    - Linear Regression\n",
    "    - Neural Networks\n",
    "    - CART\n",
    "  - Classification:\n",
    "    - Logistic Regression\n",
    "    - Nearest Neighbour\n",
    "    - Boosting Algorithm\n",
    "\n",
    "- **Descriptive Modeling**\n",
    "  - Goal: understand data structure & find patterns\n",
    "  - Segmentation:\n",
    "    - Hierarchical Clustering\n",
    "    - K-means\n",
    "    - DBSCAN\n",
    "  - Rule/Sequence Mining:\n",
    "    - Apriori Algorithm\n",
    "    - FP-growth\n",
    "    - PrefixSpan\n",
    "\n",
    "- Machine Learning task\n",
    "  - Target function: f: Attributes → Class Labels L\n",
    "\n",
    "- Data Partitioning strategy\n",
    "  - training using labeled instances (“gold standard”)\n",
    "  - split data into disjuncts sets of training data and test data\n",
    "\n",
    "- Learning process\n",
    "  - classifier learns from training data\n",
    "  - performance assessment: evaluate classifier on separate test data (to avoid data leakage = classifier learns from training data)\n",
    "\n",
    "- Problems\n",
    "  - Limitation of training data\n",
    "  - Mapping Challenges: Training Set ≠ Entire Population\n",
    "  - Overfitting\n",
    "\n",
    "- Local vs. global models\n",
    "  - Local models\n",
    "    - k-Nearest Neighbors (k-NN)\n",
    "    - Focus: Local neighborhood structures\n",
    "    - Distance based decision boundary\n",
    "    - Uses nearby data points and distance-based decision boundaries for localized predictions\n",
    "  - Global models\n",
    "    - Linear/Logistic Regression; Neural Networks\n",
    "    - Single, comprehensive decision boundary\n",
    "    - Utilizes all data to create a unified decision rule for overall generalization\n",
    "\n",
    "- Trade-Off\n",
    "  - Interpretability vs Complexity\n",
    "  - Computational Speed\n",
    "  - Data Requirements\n",
    "\n",
    "- Parametric vs. Non-Parametric Models\n",
    "  - Parametric Models\n",
    "    - Logistic Regression\n",
    "    - Finite parameter set\n",
    "    - Strong distributional assumptions\n",
    "  - Non-Parametric Models\n",
    "    - Decision Trees\n",
    "    - Random Forests\n",
    "    - Minimal data distribution assumptions\n",
    "\n",
    "- Model Complexity Spectrum\n",
    "  - “Easy” models → High interpretability\n",
    "    - Linear Regression\n",
    "    - Logistic Regression\n",
    "    - Decision Trees\n",
    "  - “Complex” models → Higher predictive accuracy\n",
    "    - Random Forests\n",
    "    - Gradient Boosting\n",
    "    - Deep Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy model with high interpretability and lower prediction accuracy\n",
    "    # - Linear Regression\n",
    "    # - Logistic Regression\n",
    "    # - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc47110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex model with lower interpretability but higheer prediction accuracy\n",
    "    # - XGBoost\n",
    "    # - LightGBM   \n",
    "    # - Deep Neual Network\n",
    "    # - Random Forest\n",
    "    # - Support Vector Machine\n",
    "    # - Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679fd8c2",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1964d75",
   "metadata": {},
   "source": [
    "**Task**\n",
    "- Present evaluation metrics for both models\n",
    "- Compare model performance and explain results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35480266",
   "metadata": {},
   "source": [
    "Evaluation Strategies:\n",
    "- Confusion Matrix\n",
    "    - Accuracy: Overall correctness of the model\n",
    "    - Precision (Positive Predictive Value): How many predicted positives are actually positive\n",
    "    - Recall (Sensitivity, True Positive Rate): How many actual positives were correctly predicted\n",
    "    - Specificity (True Negative Rate): How many actual negatives were correctly predicted\n",
    "    - F1 Score: Harmonic mean of precision and recall (balance between the two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac9251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Easy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ef783",
   "metadata": {},
   "source": [
    "- Precision-Recall (PR) Curve\n",
    "  - The PR curve plots Precision (y-axis) vs. Recall (x-axis) at different thresholds\n",
    "  - Focuses on performance in imbalanced datasets, where the positive class is rare\n",
    "\n",
    "- Area Under the PR Curve (AUC-PR)\n",
    "  - Measures the average trade-off between precision and recall\n",
    "  - Value range: 0 to 1 → Higher = Better\n",
    "  - More informative than ROC AUC for imbalanced classes, since it doesn’t account for true negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aadb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR & AUC-PR easy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR & AUC-PR complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0391e8",
   "metadata": {},
   "source": [
    "- ROC Curve (Receiver Operating Characteristic)\n",
    "  - Plots True Positive Rate (Recall) vs. False Positive Rate (FPR)\n",
    "  - Shows model performance across all classification thresholds\n",
    "  - Each point on the curve corresponds to a different threshold\n",
    "\n",
    "- Area Under the ROC Curve (AUC-ROC)\n",
    "  - AUC = Area under the ROC Curve (value between 0 and 1)\n",
    "  - Measures the model’s ability to distinguish between classes\n",
    "  - Interpretation:\n",
    "    - 1.0 = perfect classifier\n",
    "    - 0.5 = random guessing\n",
    "    - < 0.5 = worse than random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC & AUC ROC easy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de4ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC & AUC ROC complex model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bi_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
